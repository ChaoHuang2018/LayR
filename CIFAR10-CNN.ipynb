{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:1 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:2 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:3 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1\n",
      "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1\n",
      "/job:localhost/replica:0/task:0/device:GPU:2 -> device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "/job:localhost/replica:0/task:0/device:GPU:3 -> device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0203 10:05:01.989426 139912280430400 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0203 10:05:01.993028 139912280430400 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0203 10:05:01.997281 139912280430400 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(4, 4),\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 input_shape=x_train.shape[1:]\n",
    "                ))\n",
    "model.add(Conv2D(32, (4, 4), strides=2, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0203 10:05:02.764324 139912280430400 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0203 10:05:02.771892 139912280430400 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initiate Adam optimizer\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0203 10:05:04.350294 139912280430400 deprecation.py:323] From /home/jiameng/packages/Bernsp/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0203 10:05:04.412048 139912280430400 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0203 10:05:04.582833 139912280430400 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0203 10:05:04.773278 139912280430400 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0203 10:05:05.208548 139912280430400 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0203 10:05:05.212806 139912280430400 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0203 10:05:05.216601 139912280430400 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0203 10:05:05.720453 139912280430400 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 28s 142ms/step - loss: 2.2855 - acc: 0.1180 - val_loss: 2.2243 - val_acc: 0.1564\n",
      "Epoch 2/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 2.1071 - acc: 0.1813 - val_loss: 1.9893 - val_acc: 0.2136\n",
      "Epoch 3/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 1.9465 - acc: 0.2153 - val_loss: 1.8654 - val_acc: 0.2340\n",
      "Epoch 4/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 1.8739 - acc: 0.2482 - val_loss: 1.8584 - val_acc: 0.2593\n",
      "Epoch 5/100\n",
      "196/196 [==============================] - 27s 137ms/step - loss: 1.8357 - acc: 0.2762 - val_loss: 1.7900 - val_acc: 0.2839\n",
      "Epoch 6/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 1.7996 - acc: 0.2931 - val_loss: 1.7305 - val_acc: 0.3175\n",
      "Epoch 7/100\n",
      "196/196 [==============================] - 27s 137ms/step - loss: 1.7683 - acc: 0.3161 - val_loss: 1.7232 - val_acc: 0.3467\n",
      "Epoch 8/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.7427 - acc: 0.3306 - val_loss: 1.7175 - val_acc: 0.3495\n",
      "Epoch 9/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 1.7266 - acc: 0.3393 - val_loss: 1.6955 - val_acc: 0.3583\n",
      "Epoch 10/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 1.7093 - acc: 0.3482 - val_loss: 1.6681 - val_acc: 0.3689\n",
      "Epoch 11/100\n",
      "196/196 [==============================] - 26s 132ms/step - loss: 1.7031 - acc: 0.3559 - val_loss: 1.6482 - val_acc: 0.3820\n",
      "Epoch 12/100\n",
      "196/196 [==============================] - 24s 124ms/step - loss: 1.6785 - acc: 0.3642 - val_loss: 1.6524 - val_acc: 0.3768\n",
      "Epoch 13/100\n",
      "196/196 [==============================] - 25s 130ms/step - loss: 1.6680 - acc: 0.3678 - val_loss: 1.6221 - val_acc: 0.3875\n",
      "Epoch 14/100\n",
      "196/196 [==============================] - 25s 129ms/step - loss: 1.6640 - acc: 0.3750 - val_loss: 1.6332 - val_acc: 0.3842\n",
      "Epoch 15/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 1.6461 - acc: 0.3802 - val_loss: 1.6483 - val_acc: 0.3805\n",
      "Epoch 16/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 1.6317 - acc: 0.3897 - val_loss: 1.5918 - val_acc: 0.4071\n",
      "Epoch 17/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 1.6177 - acc: 0.3974 - val_loss: 1.5925 - val_acc: 0.4044\n",
      "Epoch 18/100\n",
      "196/196 [==============================] - 26s 132ms/step - loss: 1.6110 - acc: 0.3987 - val_loss: 1.5673 - val_acc: 0.4157\n",
      "Epoch 19/100\n",
      "196/196 [==============================] - 26s 131ms/step - loss: 1.5919 - acc: 0.4067 - val_loss: 1.6090 - val_acc: 0.4073\n",
      "Epoch 20/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.5787 - acc: 0.4148 - val_loss: 1.5735 - val_acc: 0.4225\n",
      "Epoch 21/100\n",
      "196/196 [==============================] - 25s 129ms/step - loss: 1.5739 - acc: 0.4169 - val_loss: 1.5648 - val_acc: 0.4303\n",
      "Epoch 22/100\n",
      "196/196 [==============================] - 26s 131ms/step - loss: 1.5551 - acc: 0.4276 - val_loss: 1.5260 - val_acc: 0.4386\n",
      "Epoch 23/100\n",
      "196/196 [==============================] - 25s 129ms/step - loss: 1.5415 - acc: 0.4322 - val_loss: 1.5235 - val_acc: 0.4420\n",
      "Epoch 24/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 1.5378 - acc: 0.4316 - val_loss: 1.5186 - val_acc: 0.4367\n",
      "Epoch 25/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 1.5097 - acc: 0.4421 - val_loss: 1.4979 - val_acc: 0.4427\n",
      "Epoch 26/100\n",
      "196/196 [==============================] - 26s 131ms/step - loss: 1.4101 - acc: 0.4806 - val_loss: 1.3596 - val_acc: 0.4970\n",
      "Epoch 27/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.3594 - acc: 0.4988 - val_loss: 1.3019 - val_acc: 0.5224\n",
      "Epoch 28/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.3316 - acc: 0.5091 - val_loss: 1.2904 - val_acc: 0.5328\n",
      "Epoch 29/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.3045 - acc: 0.5217 - val_loss: 1.2931 - val_acc: 0.5300\n",
      "Epoch 30/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.2882 - acc: 0.5271 - val_loss: 1.2515 - val_acc: 0.5434\n",
      "Epoch 31/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 1.2651 - acc: 0.5374 - val_loss: 1.2832 - val_acc: 0.5348\n",
      "Epoch 32/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.2423 - acc: 0.5467 - val_loss: 1.2369 - val_acc: 0.5464\n",
      "Epoch 33/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.2400 - acc: 0.5442 - val_loss: 1.2468 - val_acc: 0.5549\n",
      "Epoch 34/100\n",
      "196/196 [==============================] - 24s 120ms/step - loss: 1.2183 - acc: 0.5561 - val_loss: 1.2065 - val_acc: 0.5641\n",
      "Epoch 35/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.2097 - acc: 0.5581 - val_loss: 1.1856 - val_acc: 0.5708\n",
      "Epoch 36/100\n",
      "196/196 [==============================] - 27s 135ms/step - loss: 1.1943 - acc: 0.5624 - val_loss: 1.1726 - val_acc: 0.5828\n",
      "Epoch 37/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.1852 - acc: 0.5689 - val_loss: 1.1887 - val_acc: 0.5675\n",
      "Epoch 38/100\n",
      "196/196 [==============================] - 27s 135ms/step - loss: 1.1691 - acc: 0.5744 - val_loss: 1.1812 - val_acc: 0.5774\n",
      "Epoch 39/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 1.1664 - acc: 0.5778 - val_loss: 1.1815 - val_acc: 0.5762\n",
      "Epoch 40/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.1566 - acc: 0.5800 - val_loss: 1.1657 - val_acc: 0.5813\n",
      "Epoch 41/100\n",
      "196/196 [==============================] - 24s 121ms/step - loss: 1.1353 - acc: 0.5883 - val_loss: 1.1737 - val_acc: 0.5835\n",
      "Epoch 42/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 1.1290 - acc: 0.5879 - val_loss: 1.1692 - val_acc: 0.5831\n",
      "Epoch 43/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.1231 - acc: 0.5929 - val_loss: 1.1430 - val_acc: 0.5932\n",
      "Epoch 44/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 1.1152 - acc: 0.5950 - val_loss: 1.1248 - val_acc: 0.5971\n",
      "Epoch 45/100\n",
      "196/196 [==============================] - 25s 128ms/step - loss: 1.1077 - acc: 0.6004 - val_loss: 1.1270 - val_acc: 0.5977\n",
      "Epoch 46/100\n",
      "196/196 [==============================] - 25s 127ms/step - loss: 1.1054 - acc: 0.6009 - val_loss: 1.1293 - val_acc: 0.6035\n",
      "Epoch 47/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.0871 - acc: 0.6046 - val_loss: 1.1102 - val_acc: 0.6067\n",
      "Epoch 48/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.0779 - acc: 0.6118 - val_loss: 1.1006 - val_acc: 0.6133\n",
      "Epoch 49/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.0699 - acc: 0.6142 - val_loss: 1.1003 - val_acc: 0.6079\n",
      "Epoch 50/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.0679 - acc: 0.6149 - val_loss: 1.1123 - val_acc: 0.6072\n",
      "Epoch 51/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.0603 - acc: 0.6195 - val_loss: 1.0867 - val_acc: 0.6134\n",
      "Epoch 52/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.0445 - acc: 0.6256 - val_loss: 1.0888 - val_acc: 0.6153\n",
      "Epoch 53/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 1.0397 - acc: 0.6283 - val_loss: 1.0955 - val_acc: 0.6109\n",
      "Epoch 54/100\n",
      "196/196 [==============================] - 24s 121ms/step - loss: 1.0452 - acc: 0.6249 - val_loss: 1.1140 - val_acc: 0.6050\n",
      "Epoch 55/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 1.0290 - acc: 0.6319 - val_loss: 1.0708 - val_acc: 0.6253\n",
      "Epoch 56/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 1.0259 - acc: 0.6327 - val_loss: 1.1351 - val_acc: 0.6114\n",
      "Epoch 57/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.0231 - acc: 0.6332 - val_loss: 1.0628 - val_acc: 0.6299\n",
      "Epoch 58/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.0163 - acc: 0.6390 - val_loss: 1.0459 - val_acc: 0.6291\n",
      "Epoch 59/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 1.0105 - acc: 0.6391 - val_loss: 1.0627 - val_acc: 0.6310\n",
      "Epoch 60/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.0021 - acc: 0.6438 - val_loss: 1.1019 - val_acc: 0.6170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 0.9939 - acc: 0.6467 - val_loss: 1.0362 - val_acc: 0.6366\n",
      "Epoch 62/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 0.9941 - acc: 0.6463 - val_loss: 1.0411 - val_acc: 0.6389\n",
      "Epoch 63/100\n",
      "196/196 [==============================] - 25s 129ms/step - loss: 0.9859 - acc: 0.6510 - val_loss: 1.0292 - val_acc: 0.6440\n",
      "Epoch 64/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.9796 - acc: 0.6536 - val_loss: 1.0694 - val_acc: 0.6326\n",
      "Epoch 65/100\n",
      "196/196 [==============================] - 24s 122ms/step - loss: 0.9785 - acc: 0.6539 - val_loss: 1.0618 - val_acc: 0.6308\n",
      "Epoch 66/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.9694 - acc: 0.6566 - val_loss: 1.0473 - val_acc: 0.6402\n",
      "Epoch 67/100\n",
      "196/196 [==============================] - 26s 131ms/step - loss: 0.9595 - acc: 0.6602 - val_loss: 1.0881 - val_acc: 0.6254\n",
      "Epoch 68/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.9607 - acc: 0.6605 - val_loss: 1.0274 - val_acc: 0.6476\n",
      "Epoch 69/100\n",
      "196/196 [==============================] - 26s 132ms/step - loss: 0.9605 - acc: 0.6616 - val_loss: 1.0906 - val_acc: 0.6249\n",
      "Epoch 70/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.9550 - acc: 0.6631 - val_loss: 1.0586 - val_acc: 0.6456\n",
      "Epoch 71/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.9540 - acc: 0.6625 - val_loss: 1.0281 - val_acc: 0.6468\n",
      "Epoch 72/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 0.9506 - acc: 0.6674 - val_loss: 1.0323 - val_acc: 0.6476\n",
      "Epoch 73/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.9354 - acc: 0.6712 - val_loss: 1.0494 - val_acc: 0.6469\n",
      "Epoch 74/100\n",
      "196/196 [==============================] - 27s 135ms/step - loss: 0.9368 - acc: 0.6715 - val_loss: 1.0392 - val_acc: 0.6438\n",
      "Epoch 75/100\n",
      "196/196 [==============================] - 27s 135ms/step - loss: 0.9287 - acc: 0.6757 - val_loss: 1.0383 - val_acc: 0.6475\n",
      "Epoch 76/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 0.9313 - acc: 0.6732 - val_loss: 1.0674 - val_acc: 0.6363\n",
      "Epoch 77/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.9246 - acc: 0.6748 - val_loss: 1.0488 - val_acc: 0.6464\n",
      "Epoch 78/100\n",
      "196/196 [==============================] - 27s 135ms/step - loss: 0.9168 - acc: 0.6776 - val_loss: 1.0253 - val_acc: 0.6523\n",
      "Epoch 79/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.9235 - acc: 0.6757 - val_loss: 0.9980 - val_acc: 0.6628\n",
      "Epoch 80/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 0.9151 - acc: 0.6800 - val_loss: 1.0249 - val_acc: 0.6511\n",
      "Epoch 81/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.9191 - acc: 0.6795 - val_loss: 1.0691 - val_acc: 0.6486\n",
      "Epoch 82/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 0.9062 - acc: 0.6821 - val_loss: 1.0587 - val_acc: 0.6431\n",
      "Epoch 83/100\n",
      "196/196 [==============================] - 27s 135ms/step - loss: 0.9029 - acc: 0.6838 - val_loss: 1.0100 - val_acc: 0.6507\n",
      "Epoch 84/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.8929 - acc: 0.6900 - val_loss: 1.0350 - val_acc: 0.6532\n",
      "Epoch 85/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.8998 - acc: 0.6880 - val_loss: 1.0033 - val_acc: 0.6600\n",
      "Epoch 86/100\n",
      "196/196 [==============================] - 27s 135ms/step - loss: 0.9030 - acc: 0.6848 - val_loss: 1.0264 - val_acc: 0.6526\n",
      "Epoch 87/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.8879 - acc: 0.6895 - val_loss: 1.1102 - val_acc: 0.6276\n",
      "Epoch 88/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.8932 - acc: 0.6893 - val_loss: 0.9912 - val_acc: 0.6661\n",
      "Epoch 89/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.8842 - acc: 0.6921 - val_loss: 0.9690 - val_acc: 0.6697\n",
      "Epoch 90/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 0.8737 - acc: 0.6978 - val_loss: 0.9885 - val_acc: 0.6679\n",
      "Epoch 91/100\n",
      "196/196 [==============================] - 27s 135ms/step - loss: 0.8633 - acc: 0.7017 - val_loss: 0.9682 - val_acc: 0.6761\n",
      "Epoch 92/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 0.8506 - acc: 0.7044 - val_loss: 1.0319 - val_acc: 0.6583\n",
      "Epoch 93/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 0.8512 - acc: 0.7037 - val_loss: 0.9904 - val_acc: 0.6721\n",
      "Epoch 94/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.8514 - acc: 0.7032 - val_loss: 0.9941 - val_acc: 0.6717\n",
      "Epoch 95/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.8492 - acc: 0.7049 - val_loss: 0.9388 - val_acc: 0.6820\n",
      "Epoch 96/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.8349 - acc: 0.7096 - val_loss: 0.9733 - val_acc: 0.6716\n",
      "Epoch 97/100\n",
      "196/196 [==============================] - 27s 135ms/step - loss: 0.8367 - acc: 0.7093 - val_loss: 0.9593 - val_acc: 0.6790\n",
      "Epoch 98/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.8282 - acc: 0.7120 - val_loss: 0.9567 - val_acc: 0.6826\n",
      "Epoch 99/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.8292 - acc: 0.7080 - val_loss: 0.9401 - val_acc: 0.6860\n",
      "Epoch 100/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 0.8177 - acc: 0.7163 - val_loss: 0.9616 - val_acc: 0.6791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f1408ed68>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    # randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.,  # set range for random shear\n",
    "    zoom_range=0.,  # set range for random zoom\n",
    "    channel_shift_range=0.,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=int(np.ceil(x_train.shape[0]/batch_size)),\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"/home/jiameng/packages/ReachNN/ReachNN-CNN/model/model_CIFAR_CNN_Medium.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"/home/jiameng/packages/ReachNN/ReachNN-CNN/model/model_CIFAR_CNN_Medium.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bernsp",
   "language": "python",
   "name": "bernsp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
