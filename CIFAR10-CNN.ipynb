{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:1 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:2 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:3 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1\n",
      "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1\n",
      "/job:localhost/replica:0/task:0/device:GPU:2 -> device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "/job:localhost/replica:0/task:0/device:GPU:3 -> device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0125 14:06:06.223568 140197268100928 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0125 14:06:06.225589 140197268100928 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0125 14:06:06.229564 140197268100928 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Conv2D(128, (3, 3), strides=2, activation='relu'))\n",
    "model.add(Conv2D(256, (5, 5), strides=2, activation='relu'))\n",
    "model.add(Conv2D(512, (5, 5), strides=1, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0125 14:06:06.803721 140197268100928 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0125 14:06:06.811183 140197268100928 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initiate Adam optimizer\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0125 14:06:09.854239 140197268100928 deprecation.py:323] From /home/jiameng/packages/Bernsp/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0125 14:06:09.952825 140197268100928 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0125 14:06:10.230289 140197268100928 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0125 14:06:10.527844 140197268100928 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0125 14:06:10.961712 140197268100928 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0125 14:06:10.966253 140197268100928 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0125 14:06:10.969501 140197268100928 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0125 14:06:11.446394 140197268100928 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 24s 122ms/step - loss: 2.2013 - acc: 0.1465 - val_loss: 1.9576 - val_acc: 0.1997\n",
      "Epoch 2/100\n",
      "196/196 [==============================] - 21s 105ms/step - loss: 1.9185 - acc: 0.2208 - val_loss: 1.8772 - val_acc: 0.2440\n",
      "Epoch 3/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 1.8349 - acc: 0.2728 - val_loss: 1.7442 - val_acc: 0.3044\n",
      "Epoch 4/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 1.7568 - acc: 0.3195 - val_loss: 1.6832 - val_acc: 0.3686\n",
      "Epoch 5/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 1.7043 - acc: 0.3486 - val_loss: 1.6106 - val_acc: 0.3872\n",
      "Epoch 6/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 1.6415 - acc: 0.3851 - val_loss: 1.6421 - val_acc: 0.3929\n",
      "Epoch 7/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 1.6070 - acc: 0.4125 - val_loss: 1.5599 - val_acc: 0.4487\n",
      "Epoch 8/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 1.5628 - acc: 0.4373 - val_loss: 1.5089 - val_acc: 0.4581\n",
      "Epoch 9/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 1.5297 - acc: 0.4551 - val_loss: 1.4895 - val_acc: 0.4897\n",
      "Epoch 10/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 1.4890 - acc: 0.4734 - val_loss: 1.4942 - val_acc: 0.4720\n",
      "Epoch 11/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 1.4666 - acc: 0.4873 - val_loss: 1.4230 - val_acc: 0.5093\n",
      "Epoch 12/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 1.4427 - acc: 0.5022 - val_loss: 1.3663 - val_acc: 0.5239\n",
      "Epoch 13/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 1.4248 - acc: 0.5089 - val_loss: 1.3709 - val_acc: 0.5328\n",
      "Epoch 14/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 1.4092 - acc: 0.5190 - val_loss: 1.3990 - val_acc: 0.5408\n",
      "Epoch 15/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 1.3779 - acc: 0.5288 - val_loss: 1.3111 - val_acc: 0.5577\n",
      "Epoch 16/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 1.3617 - acc: 0.5334 - val_loss: 1.2808 - val_acc: 0.5625\n",
      "Epoch 17/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 1.2647 - acc: 0.5587 - val_loss: 1.2404 - val_acc: 0.5658\n",
      "Epoch 18/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 1.1823 - acc: 0.5868 - val_loss: 1.1605 - val_acc: 0.5977\n",
      "Epoch 19/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 1.1343 - acc: 0.6034 - val_loss: 1.0894 - val_acc: 0.6223\n",
      "Epoch 20/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 1.1005 - acc: 0.6156 - val_loss: 1.0982 - val_acc: 0.6251\n",
      "Epoch 21/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 1.0634 - acc: 0.6296 - val_loss: 1.0649 - val_acc: 0.6396\n",
      "Epoch 22/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 1.0237 - acc: 0.6481 - val_loss: 0.9798 - val_acc: 0.6644\n",
      "Epoch 23/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.9686 - acc: 0.6671 - val_loss: 0.9704 - val_acc: 0.6668\n",
      "Epoch 24/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.9314 - acc: 0.6813 - val_loss: 0.9521 - val_acc: 0.6795\n",
      "Epoch 25/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.8992 - acc: 0.6940 - val_loss: 0.9028 - val_acc: 0.6936\n",
      "Epoch 26/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.8631 - acc: 0.7063 - val_loss: 0.8716 - val_acc: 0.7074\n",
      "Epoch 27/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.8436 - acc: 0.7119 - val_loss: 0.8811 - val_acc: 0.7076\n",
      "Epoch 28/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.8033 - acc: 0.7299 - val_loss: 0.8262 - val_acc: 0.7232\n",
      "Epoch 29/100\n",
      "196/196 [==============================] - 21s 105ms/step - loss: 0.7748 - acc: 0.7372 - val_loss: 0.8288 - val_acc: 0.7241\n",
      "Epoch 30/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 0.7647 - acc: 0.7428 - val_loss: 0.8114 - val_acc: 0.7291\n",
      "Epoch 31/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 0.7331 - acc: 0.7528 - val_loss: 0.8253 - val_acc: 0.7297\n",
      "Epoch 32/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.7330 - acc: 0.7535 - val_loss: 0.8123 - val_acc: 0.7327\n",
      "Epoch 33/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.7066 - acc: 0.7617 - val_loss: 0.8013 - val_acc: 0.7353\n",
      "Epoch 34/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.6892 - acc: 0.7662 - val_loss: 0.7832 - val_acc: 0.7426\n",
      "Epoch 35/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.6683 - acc: 0.7766 - val_loss: 0.7965 - val_acc: 0.7430\n",
      "Epoch 36/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.6554 - acc: 0.7796 - val_loss: 0.7623 - val_acc: 0.7521\n",
      "Epoch 37/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 0.6382 - acc: 0.7862 - val_loss: 0.7500 - val_acc: 0.7535\n",
      "Epoch 38/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.6262 - acc: 0.7889 - val_loss: 0.7673 - val_acc: 0.7562\n",
      "Epoch 39/100\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6173 - acc: 0.7928 - val_loss: 0.7521 - val_acc: 0.7586\n",
      "Epoch 40/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.6021 - acc: 0.7995 - val_loss: 0.7317 - val_acc: 0.7610\n",
      "Epoch 41/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.5913 - acc: 0.8035 - val_loss: 0.7433 - val_acc: 0.7631\n",
      "Epoch 42/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.5812 - acc: 0.8048 - val_loss: 0.7272 - val_acc: 0.7644\n",
      "Epoch 43/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.5708 - acc: 0.8087 - val_loss: 0.7650 - val_acc: 0.7573\n",
      "Epoch 44/100\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.5625 - acc: 0.8115 - val_loss: 0.7254 - val_acc: 0.7651\n",
      "Epoch 45/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.5519 - acc: 0.8176 - val_loss: 0.7187 - val_acc: 0.7699\n",
      "Epoch 46/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.5438 - acc: 0.8186 - val_loss: 0.7176 - val_acc: 0.7706\n",
      "Epoch 47/100\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5291 - acc: 0.8232 - val_loss: 0.6901 - val_acc: 0.7811\n",
      "Epoch 48/100\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.5138 - acc: 0.8274 - val_loss: 0.6885 - val_acc: 0.7802\n",
      "Epoch 49/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.5188 - acc: 0.8269 - val_loss: 0.6946 - val_acc: 0.7763\n",
      "Epoch 50/100\n",
      "196/196 [==============================] - 21s 105ms/step - loss: 0.5000 - acc: 0.8321 - val_loss: 0.6955 - val_acc: 0.7755\n",
      "Epoch 51/100\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.4961 - acc: 0.8329 - val_loss: 0.7117 - val_acc: 0.7767\n",
      "Epoch 52/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.5007 - acc: 0.8325 - val_loss: 0.6970 - val_acc: 0.7824\n",
      "Epoch 53/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.4680 - acc: 0.8445 - val_loss: 0.6940 - val_acc: 0.7844\n",
      "Epoch 54/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.4697 - acc: 0.8442 - val_loss: 0.6892 - val_acc: 0.7849\n",
      "Epoch 55/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.4645 - acc: 0.8450 - val_loss: 0.6914 - val_acc: 0.7860\n",
      "Epoch 56/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.4570 - acc: 0.8492 - val_loss: 0.7040 - val_acc: 0.7835\n",
      "Epoch 57/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.4527 - acc: 0.8505 - val_loss: 0.7259 - val_acc: 0.7744\n",
      "Epoch 58/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.4420 - acc: 0.8529 - val_loss: 0.7021 - val_acc: 0.7852\n",
      "Epoch 59/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.4395 - acc: 0.8546 - val_loss: 0.7294 - val_acc: 0.7774\n",
      "Epoch 60/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.4314 - acc: 0.8567 - val_loss: 0.7049 - val_acc: 0.7861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.4287 - acc: 0.8571 - val_loss: 0.6809 - val_acc: 0.7895\n",
      "Epoch 62/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.4162 - acc: 0.8621 - val_loss: 0.7078 - val_acc: 0.7825\n",
      "Epoch 63/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 0.4197 - acc: 0.8596 - val_loss: 0.7102 - val_acc: 0.7845\n",
      "Epoch 64/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 0.4086 - acc: 0.8628 - val_loss: 0.6767 - val_acc: 0.7921\n",
      "Epoch 65/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 0.4020 - acc: 0.8671 - val_loss: 0.7123 - val_acc: 0.7857\n",
      "Epoch 66/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3991 - acc: 0.8682 - val_loss: 0.7207 - val_acc: 0.7812\n",
      "Epoch 67/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.4014 - acc: 0.8663 - val_loss: 0.6962 - val_acc: 0.7890\n",
      "Epoch 68/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.3931 - acc: 0.8704 - val_loss: 0.6818 - val_acc: 0.7909\n",
      "Epoch 69/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 0.3863 - acc: 0.8718 - val_loss: 0.6803 - val_acc: 0.7915\n",
      "Epoch 70/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 0.3866 - acc: 0.8712 - val_loss: 0.7284 - val_acc: 0.7888\n",
      "Epoch 71/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3810 - acc: 0.8740 - val_loss: 0.7353 - val_acc: 0.7801\n",
      "Epoch 72/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3718 - acc: 0.8762 - val_loss: 0.6958 - val_acc: 0.7942\n",
      "Epoch 73/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3689 - acc: 0.8770 - val_loss: 0.6720 - val_acc: 0.7981\n",
      "Epoch 74/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3645 - acc: 0.8781 - val_loss: 0.6973 - val_acc: 0.7981\n",
      "Epoch 75/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3579 - acc: 0.8806 - val_loss: 0.6944 - val_acc: 0.7900\n",
      "Epoch 76/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3536 - acc: 0.8828 - val_loss: 0.6966 - val_acc: 0.7965\n",
      "Epoch 77/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.3580 - acc: 0.8808 - val_loss: 0.7148 - val_acc: 0.7867\n",
      "Epoch 78/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.3555 - acc: 0.8830 - val_loss: 0.7045 - val_acc: 0.7910\n",
      "Epoch 79/100\n",
      "196/196 [==============================] - 21s 105ms/step - loss: 0.3479 - acc: 0.8851 - val_loss: 0.6996 - val_acc: 0.7942\n",
      "Epoch 80/100\n",
      "196/196 [==============================] - 21s 105ms/step - loss: 0.3400 - acc: 0.8871 - val_loss: 0.7110 - val_acc: 0.7903\n",
      "Epoch 81/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3354 - acc: 0.8884 - val_loss: 0.6921 - val_acc: 0.7960\n",
      "Epoch 82/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.3343 - acc: 0.8904 - val_loss: 0.7115 - val_acc: 0.7953\n",
      "Epoch 83/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.3331 - acc: 0.8899 - val_loss: 0.7449 - val_acc: 0.7922\n",
      "Epoch 84/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.3365 - acc: 0.8892 - val_loss: 0.7491 - val_acc: 0.7821\n",
      "Epoch 85/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3229 - acc: 0.8953 - val_loss: 0.7438 - val_acc: 0.7897\n",
      "Epoch 86/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3210 - acc: 0.8940 - val_loss: 0.7109 - val_acc: 0.7958\n",
      "Epoch 87/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 0.3149 - acc: 0.8962 - val_loss: 0.7204 - val_acc: 0.7981\n",
      "Epoch 88/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.3152 - acc: 0.8959 - val_loss: 0.7012 - val_acc: 0.8001\n",
      "Epoch 89/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.3132 - acc: 0.8976 - val_loss: 0.7316 - val_acc: 0.7963\n",
      "Epoch 90/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3186 - acc: 0.8948 - val_loss: 0.6979 - val_acc: 0.7986\n",
      "Epoch 91/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.3200 - acc: 0.8949 - val_loss: 0.6829 - val_acc: 0.8017\n",
      "Epoch 92/100\n",
      "196/196 [==============================] - 21s 107ms/step - loss: 0.3066 - acc: 0.8991 - val_loss: 0.7406 - val_acc: 0.7918\n",
      "Epoch 93/100\n",
      "196/196 [==============================] - 27s 140ms/step - loss: 0.3054 - acc: 0.9001 - val_loss: 0.7349 - val_acc: 0.7896\n",
      "Epoch 94/100\n",
      "196/196 [==============================] - 25s 126ms/step - loss: 0.3014 - acc: 0.8996 - val_loss: 0.6825 - val_acc: 0.8051\n",
      "Epoch 95/100\n",
      "196/196 [==============================] - 21s 105ms/step - loss: 0.2973 - acc: 0.9025 - val_loss: 0.7367 - val_acc: 0.7979\n",
      "Epoch 96/100\n",
      "196/196 [==============================] - 20s 103ms/step - loss: 0.2885 - acc: 0.9046 - val_loss: 0.7132 - val_acc: 0.7982\n",
      "Epoch 97/100\n",
      "196/196 [==============================] - 21s 109ms/step - loss: 0.3014 - acc: 0.9001 - val_loss: 0.7022 - val_acc: 0.7989\n",
      "Epoch 98/100\n",
      "196/196 [==============================] - 21s 108ms/step - loss: 0.2901 - acc: 0.9039 - val_loss: 0.6843 - val_acc: 0.8052\n",
      "Epoch 99/100\n",
      "196/196 [==============================] - 21s 106ms/step - loss: 0.2880 - acc: 0.9052 - val_loss: 0.6993 - val_acc: 0.7984\n",
      "Epoch 100/100\n",
      "119/196 [=================>............] - ETA: 8s - loss: 0.2792 - acc: 0.9067"
     ]
    }
   ],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    # randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.,  # set range for random shear\n",
    "    zoom_range=0.,  # set range for random zoom\n",
    "    channel_shift_range=0.,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=int(np.ceil(x_train.shape[0]/batch_size)),\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"/home/jiameng/packages/ReachNN/ReachNN-CNN/model/model_CIFAR_CNN_Small.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"/home/jiameng/packages/ReachNN/ReachNN-CNN/model/model_CIFAR_CNN_Small.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bernsp",
   "language": "python",
   "name": "bernsp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
