{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:1 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:2 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:3 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1\n",
      "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1\n",
      "/job:localhost/replica:0/task:0/device:GPU:2 -> device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "/job:localhost/replica:0/task:0/device:GPU:3 -> device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0204 15:09:31.662342 140099731035968 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0204 15:09:31.665385 140099731035968 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0204 15:09:31.670738 140099731035968 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 input_shape=x_train.shape[1:]\n",
    "                ))\n",
    "model.add(Conv2D(32, (3, 3), strides=1, activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=2, activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=1, activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=1, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0204 15:09:32.796570 140099731035968 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0204 15:09:32.803029 140099731035968 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initiate Adam optimizer\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0204 15:09:39.201055 140099731035968 deprecation.py:323] From /home/jiameng/packages/Bernsp/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0204 15:09:39.262440 140099731035968 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0204 15:09:39.427077 140099731035968 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0204 15:09:39.611423 140099731035968 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0204 15:09:40.117464 140099731035968 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0204 15:09:40.119988 140099731035968 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0204 15:09:40.125295 140099731035968 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0204 15:09:40.663297 140099731035968 module_wrapper.py:139] From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 28s 144ms/step - loss: 2.1553 - acc: 0.1521 - val_loss: 1.9163 - val_acc: 0.2329\n",
      "Epoch 2/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.9052 - acc: 0.2322 - val_loss: 1.8495 - val_acc: 0.2663\n",
      "Epoch 3/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.8282 - acc: 0.2906 - val_loss: 1.7543 - val_acc: 0.3185\n",
      "Epoch 4/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 1.7424 - acc: 0.3303 - val_loss: 1.6195 - val_acc: 0.3742\n",
      "Epoch 5/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 1.6085 - acc: 0.3903 - val_loss: 1.5237 - val_acc: 0.4277\n",
      "Epoch 6/100\n",
      "196/196 [==============================] - 26s 132ms/step - loss: 1.4917 - acc: 0.4464 - val_loss: 1.3900 - val_acc: 0.4964\n",
      "Epoch 7/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.3836 - acc: 0.4878 - val_loss: 1.2798 - val_acc: 0.5331\n",
      "Epoch 8/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 1.2878 - acc: 0.5263 - val_loss: 1.2193 - val_acc: 0.5501\n",
      "Epoch 9/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.2190 - acc: 0.5571 - val_loss: 1.1791 - val_acc: 0.5763\n",
      "Epoch 10/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.1473 - acc: 0.5845 - val_loss: 1.1781 - val_acc: 0.5770\n",
      "Epoch 11/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.0973 - acc: 0.6083 - val_loss: 1.0422 - val_acc: 0.6322\n",
      "Epoch 12/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 1.0450 - acc: 0.6267 - val_loss: 1.0354 - val_acc: 0.6317\n",
      "Epoch 13/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.9948 - acc: 0.6457 - val_loss: 0.9701 - val_acc: 0.6595\n",
      "Epoch 14/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.9664 - acc: 0.6577 - val_loss: 1.0208 - val_acc: 0.6477\n",
      "Epoch 15/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.9260 - acc: 0.6746 - val_loss: 0.8972 - val_acc: 0.6864\n",
      "Epoch 16/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.8952 - acc: 0.6839 - val_loss: 0.9590 - val_acc: 0.6722\n",
      "Epoch 17/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.8664 - acc: 0.6936 - val_loss: 0.8772 - val_acc: 0.6997\n",
      "Epoch 18/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.8498 - acc: 0.7011 - val_loss: 0.8739 - val_acc: 0.6980\n",
      "Epoch 19/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.8261 - acc: 0.7104 - val_loss: 0.8168 - val_acc: 0.7239\n",
      "Epoch 20/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.8089 - acc: 0.7162 - val_loss: 0.8545 - val_acc: 0.7067\n",
      "Epoch 21/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.7824 - acc: 0.7266 - val_loss: 0.8315 - val_acc: 0.7149\n",
      "Epoch 22/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.7686 - acc: 0.7318 - val_loss: 0.8194 - val_acc: 0.7188\n",
      "Epoch 23/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.7467 - acc: 0.7373 - val_loss: 0.8156 - val_acc: 0.7264\n",
      "Epoch 24/100\n",
      "196/196 [==============================] - 27s 135ms/step - loss: 0.7399 - acc: 0.7431 - val_loss: 0.7767 - val_acc: 0.7366\n",
      "Epoch 25/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.7287 - acc: 0.7449 - val_loss: 0.8000 - val_acc: 0.7312\n",
      "Epoch 26/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.7207 - acc: 0.7477 - val_loss: 0.8158 - val_acc: 0.7258\n",
      "Epoch 27/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.7104 - acc: 0.7532 - val_loss: 0.7623 - val_acc: 0.7430\n",
      "Epoch 28/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.6908 - acc: 0.7604 - val_loss: 0.7369 - val_acc: 0.7553\n",
      "Epoch 29/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.6869 - acc: 0.7625 - val_loss: 0.7418 - val_acc: 0.7502\n",
      "Epoch 30/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.6740 - acc: 0.7675 - val_loss: 0.7484 - val_acc: 0.7457\n",
      "Epoch 31/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.6570 - acc: 0.7735 - val_loss: 0.7625 - val_acc: 0.7454\n",
      "Epoch 32/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.6514 - acc: 0.7723 - val_loss: 0.7642 - val_acc: 0.7472\n",
      "Epoch 33/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.6390 - acc: 0.7785 - val_loss: 0.7267 - val_acc: 0.7631\n",
      "Epoch 34/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.6357 - acc: 0.7796 - val_loss: 0.7482 - val_acc: 0.7527\n",
      "Epoch 35/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.6274 - acc: 0.7847 - val_loss: 0.7168 - val_acc: 0.7614\n",
      "Epoch 36/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.6237 - acc: 0.7851 - val_loss: 0.7256 - val_acc: 0.7633\n",
      "Epoch 37/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.6079 - acc: 0.7908 - val_loss: 0.6853 - val_acc: 0.7688\n",
      "Epoch 38/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.6000 - acc: 0.7942 - val_loss: 0.6832 - val_acc: 0.7734\n",
      "Epoch 39/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.5984 - acc: 0.7935 - val_loss: 0.7226 - val_acc: 0.7617\n",
      "Epoch 40/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.5988 - acc: 0.7945 - val_loss: 0.7051 - val_acc: 0.7669\n",
      "Epoch 41/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.5837 - acc: 0.8000 - val_loss: 0.6954 - val_acc: 0.7691\n",
      "Epoch 42/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.5696 - acc: 0.8055 - val_loss: 0.6977 - val_acc: 0.7708\n",
      "Epoch 43/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.5759 - acc: 0.8028 - val_loss: 0.7403 - val_acc: 0.7567\n",
      "Epoch 44/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.5587 - acc: 0.8084 - val_loss: 0.7404 - val_acc: 0.7592\n",
      "Epoch 45/100\n",
      "196/196 [==============================] - 26s 132ms/step - loss: 0.5602 - acc: 0.8081 - val_loss: 0.7602 - val_acc: 0.7567\n",
      "Epoch 46/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.5548 - acc: 0.8091 - val_loss: 0.6549 - val_acc: 0.7798\n",
      "Epoch 47/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.5475 - acc: 0.8136 - val_loss: 0.7033 - val_acc: 0.7700\n",
      "Epoch 48/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.5408 - acc: 0.8151 - val_loss: 0.6560 - val_acc: 0.7809\n",
      "Epoch 49/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.5338 - acc: 0.8162 - val_loss: 0.7772 - val_acc: 0.7524\n",
      "Epoch 50/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.5294 - acc: 0.8185 - val_loss: 0.6332 - val_acc: 0.7939\n",
      "Epoch 51/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.5285 - acc: 0.8195 - val_loss: 0.6585 - val_acc: 0.7823\n",
      "Epoch 52/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.5259 - acc: 0.8199 - val_loss: 0.6500 - val_acc: 0.7877\n",
      "Epoch 53/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.5111 - acc: 0.8260 - val_loss: 0.7116 - val_acc: 0.7739\n",
      "Epoch 54/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.5161 - acc: 0.8232 - val_loss: 0.6493 - val_acc: 0.7887\n",
      "Epoch 55/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4990 - acc: 0.8289 - val_loss: 0.6644 - val_acc: 0.7819\n",
      "Epoch 56/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.5025 - acc: 0.8267 - val_loss: 0.6404 - val_acc: 0.7927\n",
      "Epoch 57/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4976 - acc: 0.8307 - val_loss: 0.6682 - val_acc: 0.7844\n",
      "Epoch 58/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4938 - acc: 0.8309 - val_loss: 0.6525 - val_acc: 0.7907\n",
      "Epoch 59/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.4913 - acc: 0.8316 - val_loss: 0.6277 - val_acc: 0.7959\n",
      "Epoch 60/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.4882 - acc: 0.8335 - val_loss: 0.6625 - val_acc: 0.7796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4844 - acc: 0.8340 - val_loss: 0.6234 - val_acc: 0.7967\n",
      "Epoch 62/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.4693 - acc: 0.8397 - val_loss: 0.6511 - val_acc: 0.7898\n",
      "Epoch 63/100\n",
      "196/196 [==============================] - 26s 132ms/step - loss: 0.4758 - acc: 0.8380 - val_loss: 0.6851 - val_acc: 0.7832\n",
      "Epoch 64/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.4716 - acc: 0.8402 - val_loss: 0.6636 - val_acc: 0.7842\n",
      "Epoch 65/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.4674 - acc: 0.8410 - val_loss: 0.6358 - val_acc: 0.7944\n",
      "Epoch 66/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4670 - acc: 0.8419 - val_loss: 0.6652 - val_acc: 0.7887\n",
      "Epoch 67/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.4666 - acc: 0.8411 - val_loss: 0.6152 - val_acc: 0.8014\n",
      "Epoch 68/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4570 - acc: 0.8453 - val_loss: 0.6806 - val_acc: 0.7831\n",
      "Epoch 69/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4505 - acc: 0.8466 - val_loss: 0.6513 - val_acc: 0.7856\n",
      "Epoch 70/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.4502 - acc: 0.8457 - val_loss: 0.6122 - val_acc: 0.8020\n",
      "Epoch 71/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4544 - acc: 0.8451 - val_loss: 0.6340 - val_acc: 0.7974\n",
      "Epoch 72/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4392 - acc: 0.8500 - val_loss: 0.6227 - val_acc: 0.8006\n",
      "Epoch 73/100\n",
      "196/196 [==============================] - 27s 135ms/step - loss: 0.4350 - acc: 0.8500 - val_loss: 0.6345 - val_acc: 0.7979\n",
      "Epoch 74/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4407 - acc: 0.8494 - val_loss: 0.6401 - val_acc: 0.7955\n",
      "Epoch 75/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.4362 - acc: 0.8488 - val_loss: 0.6193 - val_acc: 0.8037\n",
      "Epoch 76/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4224 - acc: 0.8550 - val_loss: 0.6293 - val_acc: 0.8036\n",
      "Epoch 77/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.4310 - acc: 0.8528 - val_loss: 0.6088 - val_acc: 0.8006\n",
      "Epoch 78/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4318 - acc: 0.8520 - val_loss: 0.6184 - val_acc: 0.8010\n",
      "Epoch 79/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4207 - acc: 0.8559 - val_loss: 0.6136 - val_acc: 0.8044\n",
      "Epoch 80/100\n",
      "196/196 [==============================] - 27s 136ms/step - loss: 0.4230 - acc: 0.8572 - val_loss: 0.6573 - val_acc: 0.7915\n",
      "Epoch 81/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.4162 - acc: 0.8593 - val_loss: 0.5938 - val_acc: 0.8044\n",
      "Epoch 82/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.4114 - acc: 0.8610 - val_loss: 0.6025 - val_acc: 0.8062\n",
      "Epoch 83/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4148 - acc: 0.8586 - val_loss: 0.6173 - val_acc: 0.8034\n",
      "Epoch 84/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4101 - acc: 0.8608 - val_loss: 0.5923 - val_acc: 0.8118\n",
      "Epoch 85/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.4111 - acc: 0.8591 - val_loss: 0.6159 - val_acc: 0.8027\n",
      "Epoch 86/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.4050 - acc: 0.8609 - val_loss: 0.6260 - val_acc: 0.8030\n",
      "Epoch 87/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.4065 - acc: 0.8622 - val_loss: 0.6043 - val_acc: 0.8058\n",
      "Epoch 88/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.3999 - acc: 0.8636 - val_loss: 0.6057 - val_acc: 0.8100\n",
      "Epoch 89/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.4001 - acc: 0.8631 - val_loss: 0.6125 - val_acc: 0.8043\n",
      "Epoch 90/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.3971 - acc: 0.8636 - val_loss: 0.5986 - val_acc: 0.8102\n",
      "Epoch 91/100\n",
      "196/196 [==============================] - 26s 132ms/step - loss: 0.3902 - acc: 0.8664 - val_loss: 0.5952 - val_acc: 0.8107\n",
      "Epoch 92/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.3889 - acc: 0.8669 - val_loss: 0.5965 - val_acc: 0.8125\n",
      "Epoch 93/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.3945 - acc: 0.8632 - val_loss: 0.6024 - val_acc: 0.8091\n",
      "Epoch 94/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.3987 - acc: 0.8635 - val_loss: 0.6257 - val_acc: 0.8021\n",
      "Epoch 95/100\n",
      "196/196 [==============================] - 23s 118ms/step - loss: 0.3827 - acc: 0.8693 - val_loss: 0.5863 - val_acc: 0.8113\n",
      "Epoch 96/100\n",
      "196/196 [==============================] - 26s 135ms/step - loss: 0.3857 - acc: 0.8682 - val_loss: 0.6054 - val_acc: 0.8041\n",
      "Epoch 97/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.3802 - acc: 0.8702 - val_loss: 0.6343 - val_acc: 0.8014\n",
      "Epoch 98/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.3814 - acc: 0.8694 - val_loss: 0.5965 - val_acc: 0.8117\n",
      "Epoch 99/100\n",
      "196/196 [==============================] - 26s 133ms/step - loss: 0.3816 - acc: 0.8689 - val_loss: 0.5828 - val_acc: 0.8169\n",
      "Epoch 100/100\n",
      "196/196 [==============================] - 26s 134ms/step - loss: 0.3816 - acc: 0.8690 - val_loss: 0.5870 - val_acc: 0.8106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ac8077c88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    # randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.,  # set range for random shear\n",
    "    zoom_range=0.,  # set range for random zoom\n",
    "    channel_shift_range=0.,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=int(np.ceil(x_train.shape[0]/batch_size)),\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"/home/jiameng/packages/ReachNN/ReachNN-CNN/model/model_CIFAR_CNN_Large.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"/home/jiameng/packages/ReachNN/ReachNN-CNN/model/model_CIFAR_CNN_Large.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bernsp",
   "language": "python",
   "name": "bernsp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
